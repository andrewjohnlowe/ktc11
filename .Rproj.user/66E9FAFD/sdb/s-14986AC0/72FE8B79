{
    "collab_server" : "",
    "contents" : "############################################################\n############################################################\n#\n#\n# Here's the grand tour for making plots and tables of our data \n# to aid your interpretation and provide content for your lab \n# report. Note that your lab report must not include every \n# single plot and table that you make here. Only include ones \n# that support the claims you make in your report. Be selective!\n#\n# Here's what we're going to do\n#\n# 1. Summarise, analyse and visualise the ICP data\n#\n# 2. Summarise, analyse and visualise the LPSA data\n#\n# 3. Summarise, analyse and visualise the basic data\n#    (ie. everything else, pH, EC, SOM, CO3, etc.)\n#\n# You do need to do everything here in order, from start to \n# finish. If you get impatient, you can select several lines\n# at once (by highlighting them with the cursor) and click\n# run. You don't have do do each line one at a time, but it \n# will be easier to keep track of what you're doing if you do.\n#\n# If you encounter any error messages (keep an eye on the\n# console below for these), please let me know \n# right away with an email or text messsage. Tell me the line\n# where you got the message and I'll try to fix it. Ok, on\n# with show...\n#\n#\n############################################################\n############################################################\n# 1. Summarise, analyse and visualise the ICP data\n#\n# This code will get you from the raw data directly out of the instrument\n# to visualisations, summaries and statistics that you can use in your\n# lab report. You should have a close read of the journal articles that\n# we discussed in the ICP seminar to see how to use these data. These\n# two are good to start with:\n#\n# Misarti, N., B. P. Finney, et al. 2011. Reconstructing site \n# organization in the eastern Aleutian Islands, Alaska using \n# multi-element chemical analysis of soils. Journal of \n# Archaeological Science 38(7): 1441-1455.\n#\n# ARAUJO, A. G. M., FEATHERS, J. K., ARROYO-KALIN, M. \n# & TIZUKA, M. M. 2008. Lapa das boleiras rockshelter: \n# stratigraphy and formation processes at a paleoamerican \n# site in Central Brazil. Journal of Archaeological Science, \n# 35, 3186-3202.\n#\n# get the data from the CSV file and put it in an R object\n# ready to work on. Note that this line will pop up a window \n# that will be hidden behind the RStudio window. You may need\n# to minimise RStudio to find the window where you will select\n# the file.\n#\n# When you find the 'Select file' window, open this file:\n# KTC-ICP-data.csv\n#\nKTC_ICP <- read.csv(file.choose(), header=TRUE, stringsAsFactors = FALSE)\n#\n# This object \"KTC_ICP\" that we just made has data for both KTC \n# and BN1. Let's split it up so you can choose which site \n# you want to do the rest of the analysis on. This line\n# returns only those rows that have \"KTC\" in the Sample.ID\n# We'll come back to the replicates and blanks later\n#\nKTConly <- subset(KTC_ICP, (grepl(\"KTC\", KTC_ICP$Sample.ID))) \n# but still has blanks\nKTConly <- subset(KTConly, !(grepl(\"BLANK|Blank\", KTConly$Sample.ID))) \n# no blanks now\nKTConly <- subset(KTConly, !(grepl(\"replica\", KTConly$Sample.ID))) \n# no replicates\n#\n# And let's do the same for BN1\nBN1only <- subset(KTC_ICP, grepl(\"BN1\", KTC_ICP$Sample.ID)) \n# but still has blanks\nBN1only <- subset(BN1only, !(grepl(\"BLANK|Blank\", BN1only$Sample.ID))) \n# no blanks now\nBN1only <- subset(BN1only, !(grepl(\"replica\", BN1only$Sample.ID))) \n# no replicates\n#\n# And let's put all the blanks in one data object\n# to look at together\nblanks <- subset(KTC_ICP, grepl(\"BLANK|Blank\", KTC_ICP$Sample.ID))\n# and the replicates, ready for later\nreplicates <- subset(KTC_ICP, (grepl(\"replica\", KTC_ICP$Sample.ID)))\n#\n# From here on I'm going to use only KTC. If you want to \n# work on BN1, simply change \"KTConly\" to \"BN1only\" \n# in the following lines\n# \n# load libraries needed for plotting and manipulation\ninstall.packages(c(\"reshape2\", \"ggplot2\")) \nlibrary(reshape2)\nlibrary(ggplot2)\n#\n# convert the mg/L values, first make change them ppm to\n# ppb, then make sure they're all +ve, then take the \n# base 10 log, and put that new value into a new column\nKTConly$LogConc <- log10(abs(KTConly$Conc..Samp.1*1000))\n#\n# plot all the samples on top of each other, may take 5 seconds...\n# a fairly pointless plot, hard to see differences...\nggplot(data=KTConly, aes(group = Sample.ID)) + \n  geom_line(aes(x=Elem, y=LogConc, colour = Sample.ID))\n#\n# get ready to plot each sample separately by making a small table of just\n# sample ID, element and log concentration, and then exlude rows with NA\nKTCconcs <- as.data.frame(na.omit(cbind(as.matrix(KTConly$Sample.ID), \n                                        as.matrix(KTConly$Elem), KTConly$LogConc)))\n# give column names back again\ncolnames(KTCconcs) <- c(\"Sample.ID\", \"Element\", \"log_ugL\")\n#\n# convert mg.L from factor to numeric, ready to operate on\nKTCconcs$log_ugL <- abs(as.numeric(as.character(KTCconcs$log_ugL)))\n#\n# convert \"NA\" to zeros, \"NA\" comes from elements that were so high \n# in concentration that they saturated the detector and we have no\n# data on them at all. We can re-run the sample with different \n# settings to get data if necessary. \nKTCconcs$log_ugL <- as.numeric(ifelse(KTCconcs$log_ugL==\"NA\",\n                                      \"0\", KTCconcs$log_ugL))\n#\n# convert \"Inf\" to zero. These come from the log of a \n# negative number. The negative numbers result from \n# spectral interference during calibration\nKTCconcs$log_ugL <- as.numeric(ifelse(KTCconcs$log_ugL==\"Inf\",\n                                      \"0\", KTCconcs$log_ugL))\n#\n# re-order the sample IDs to get them in stratigraphic order\n# so they plot in that order...\n# first, find out what order they are in\nlevels(KTCconcs$Sample.ID)\n# second, reorder them, I have already determined the correct\n# order here, so it should just work when you click though\nKTCconcs$Sample.ID <- factor(KTCconcs$Sample.ID, \n                             levels(KTCconcs$Sample.ID)[c(9,2,3,1,4,5,6,8,7)])\n# check that you reordered them correctly\nlevels(KTCconcs$Sample.ID)\n#\n# Now we are going to go through a few visualisation methods\n# to try and get a look at the data and see if we can spot\n# trends and interesting patterns.\n#\n# First, plot all of the samples in a giant grid of plots... \n# this may be very slow! Could be a couple of mins...\nggplot(KTCconcs, aes(Element, log_ugL)) + geom_bar() +\n  facet_wrap( ~ Sample.ID)\n#\n# Second, let's try a single column of plots\nggplot(KTCconcs, aes(Element, log_ugL)) + geom_bar() +\n  facet_grid(Sample.ID ~ .)\n# \n# Third, let's rotate the plot to make a stratigraphic plot\n# with the samples in order from top to bottom\n#\n# We need to convert data to wide format for this plot\nlibrary(reshape2)\nKTCcast <- dcast(KTCconcs, Sample.ID ~ Element, value.var=\"log_ugL\")\n#\n# get rid of sample IDs by putting them into rownames\nrow.names(KTCcast) <- KTCcast$Sample.ID\n#\n# delete sample ID column\nKTCcast <- KTCcast[,-1]\n#\n# delete columns with zeros, these elements with problematic data\n# resulting from calibration problems or saturation of the\n# detector\nKTCcast<- KTCcast[,!(colSums(abs(KTCcast)) == 0)]\n#\n# load the packages we need to make the stratigraphic plot\ninstall.packages(\"analogue\")\nlibrary(analogue)\n#\n# make a data object that contains that sample depth \n# values in m below the surface\ndepths <- c(0.02, 0.09, 0.22, 0.30, 0.40, 0.53, 0.72, 0.92, 1.10)\n#\nKTCcast.strat <- KTCcast\n#\n# And now draw the plot...\nStratiplot(KTCcast.strat, y = depths, \n           type = c(\"h\", \"o\"), ylab = \"depth below surface (m)\",\n           varTypes = \"absolute\")\n#\n# This is a bit of a silly plot, there's too much going on.\n# But you might be able to spot some trends that you want to \n# make a note of to explore futher by plotting just a subset\n# of all the elements. \n#\n# To plot just a subset of certain elements, try this.\n# you can add as many elements as you like by\n# inserting more 'which(colnames(KTCcast.strat)==\"Na\")'\n# and just changing the element name. Don't forget the \n# comma after the close-bracket! You should go ahead and \n# replace the element names here with ones that you've found\n# to have interesting patterns.\nStratiplot(KTCcast.strat[,c(which(colnames(KTCcast.strat)==\"Sc\"), # delete or copy\n                            which(colnames(KTCcast.strat)==\"V\"), # these lines to\n                            which(colnames(KTCcast.strat)==\"Li\"),  # customise as\n                            which(colnames(KTCcast.strat)==\"Cr\"),  # customise as\n                            which(colnames(KTCcast.strat)==\"Ni\"),  # customise as\n                            which(colnames(KTCcast.strat)==\"Na\")   # you like\n                            ) ], \n           y = depths, type = c(\"h\", \"o\"), ylab = \"depth below surface (m)\",\n           varTypes = \"absolute\")\n#\n# In case you don't like the style of the Stratiplot() plot\n# here is a slightly different method of doing much the same\n# thing \n#\ninstall.packages(\"rioja\")\nlibrary(rioja)\nstrat.plot(KTCcast[,-1], y.rev = TRUE, yvar = depths) # for all elements\n#\n# To plot just a subset, try this\nstrat.plot(KTCcast.strat[,c(which(colnames(KTCcast.strat)==\"Sr\"), # delete or copy\n               which(colnames(KTCcast.strat)==\"Cr\"),        # these lines to\n               which(colnames(KTCcast.strat)==\"K\"),         # customise as\n               which(colnames(KTCcast.strat)==\"Y\")          # you like\n               ) ], y.rev = TRUE, yvar = depths) \n# \n# Right, so we've visualised the data in a \n# few ways now, let's go ahead with with two analytical methods\n# to reduce the dimensionality of the data and try to find\n# hidden structure and patterns that are not obvious when\n# we look at it all at once. \n#\n# First, we will use Principal Components Analysis, which is \n# a statistical # method for extracting relevant \n# information from confusing  data sets. The results \n# of the PCA will guide us on how to reduce the complex \n# data set to reveal the sometimes  hidden, simpli???ed \n# structure that underlies it.\n#\n# PCA is useful when we have obtained data on a large number \n# of variables (such as our elements), and believe that\n# there is some redundancy in those variables.  In this case, \n# redundancy means that some of the variables are correlated \n# with one another, possibly because they are measuring the \n# same underlying construct.  Because of this redundancy, \n# we believe that it should be possible to reduce the\n# observed variables into a smaller number of principal \n# components (artificial variables) that will account for \n# most of the variance in the observed variables.  \n# \n# Our aims with PCA are to (1) to reduce the number \n# of variables (ie. elements that deserve our attention)\n# and (2) to detect structure in the relationships \n# between variables. You should mention this in your report\n# where you use these PCA data. You'll need to find a scholary\n# source to cite on PCA, of course. \n#\n# Let's go ahead and perform the PCA...\nKTCcast.pca1 <- prcomp(~., data=KTCcast, retx=TRUE, center=TRUE, scale.=TRUE, na.action = na.omit)\n#\n# have a look at the results...\nsummary(KTCcast.pca1)\n# the 'cumulative proportion' row of the output here\n# tells us how much variation is explained by the \n# principal components as we move down the columns. Looks \n# like PC1 and PC2 capture over 80%, which is great.\n#\n# Now we will build up a plot to visualise the PCA results\n# and see the hidden structure in the data that is has found\n# for us\n#\n# extract some PCA data objects for plotting\nsd <- KTCcast.pca1$sdev\nloadings <- KTCcast.pca1$rotation\nrownames(loadings) <- colnames(KTCcast)\nscores <- KTCcast.pca1$x\n#\n# First make an empty plot window...\nplot(scores[,1], scores[,2], xlab=\"PCA 1\", ylab=\"PCA 2\", \n     type=\"n\", xlim=c(min(scores[,1:2]), max(scores[,1:2])), \n     ylim=c(min(scores[,1:2]), max(scores[,1:2])))\n#\n# You can add the loading arrows if you wish, or skip this\n# for a less cluttered plot. Loadings show which element \n# contributes to which PC axis\narrows(0,0,loadings[,1]*10,loadings[,2]*10, length=0.1, \n       angle=20, col=\"red\")\n#\n# If you added loading arrows, you probably want to label\n# them with the element names. If not, skip this\ntext(loadings[,1]*10*1.2,loadings[,2]*10*1.2, \n     rownames(loadings), col=\"red\", cex=0.7)\n#\n# Now add the points to the plot, with their sample names\n# Don't skip this! But you should experiment with making \n# the plot with and without the loading arrows. You may\n# want to include in your lab report a plot without the \n# arrows because it's less cluttered.\ntext(scores[,1],scores[,2], rownames(scores), col=\"blue\", \n     cex=0.7)\n#\n# Have a look at that plot you just made and make a note of\n# which samples seem to be close to each other. Look at the \n# loading arrows to see what elements might be relevant\n# in the groups of elements. These are details you may want to\n# discuss in your lab report.\n#\n# We can use the results of the PCA to help us make choices\n# about what specific elements to focus on for further analysis.\n# THis is the 'variable reduction' part of PCA. \n# The loadings data that the PCA generate tell us how \n# important each element is  for each PC so we can just pick\n# out the important ones and ignore the rest. This reduces the\n# number of variables we need to be concerned with.\n#\n# Let's look at the loading data from our PCA \nloadings\n#\n# Now look to see which element has the biggest negative or \n# positive value for PC1. Make that element one of the variables\n# in the plot code below. Now look in the column for PC2 and \n# find the element with the biggest -ve or +ve value\n# and put that as the second variable in the plot. Run the plot\n# code and have a look. Try changing the elements for other\n# high loading values and see what interesting groups you get. \n#\n# Now you've identified important elements in the samples,\n# you can go back to the stratigraphic plotting methods\n# and make plots with just those elements to see how just\n# those few elements change over time. #\n# \n# To explore the data with biplots of specific pairs of elements \n# note that each element has to be added twice, once to specify\n# the data point location and once to specify the label of the \n# data point\nplot(KTCcast$Fe,KTCcast$Cr,type=\"n\") # creates empty plot window\ntext(KTCcast$Fe,KTCcast$Cr,          # adds data points as sample names\n     rownames(KTCcast), col=\"blue\", cex=0.9)\n#\n# Note that your could also go back up to the Stratiplot \n# function above and plot the elements you chose from the\n# loadings table by depth, that might be very interesting also.\n#\n# Our second method for data analysis is a cluster analysis.\n# This is another method for reducing the complexity of the \n# data to see how the samples are related. This method uses\n# Ward Hierarchical Clustering, feel free to experiment \n# with other varieties, to see the the options type ?dist at\n# the R prompt, and ?hclust to see the different methods you\n# can try\n#\n# First we calculate the 'distance' of all the samples from \n# each oher\nd <- dist(KTCcast, method = \"euclidean\") # distance matrix\n#\n# Then do the cluster analysis\nfit <- hclust(d, method=\"ward\") \n#\n# And finally plot a dendrogram to visulise the distances \n# between samples.\nplot(fit) \n#\n# If you want to exclude certain elements from the cluster\n# analysis, try\n# d <- dist(KTCcast[,-which(colnames(KTCcast.strat)==\"Al\")], \n#  method = \"euclidean\")\n#\n# And see the example for the strat.plot subset to exclude\n# more than one element. \n#\n# You can refer back to the PCA plot and look at the\n# loading arrows (and the loading data) to see which \n# elements might explain how the samples are clustered.\n#\n# Third, we will make a correlation matrix of elements. This \n# is a method for summarising the data so you can identify \n# groups of elements that may be correlated. These next few lines\n# will calculate correlations between all the elements, and \n# p-values so you can assess the importance of these correlations\n# \n# We will make a table that gives the correlation values\n# and puts a star next to it if the p-value is less than \n# 0.001 (ie. worthy of interest). \n#\ninstall.packages(\"Hmisc\")\nlibrary(Hmisc)\n#\n# calculate the correlation matrix\nKTCcast.cor <- rcorr(as.matrix(KTCcast), type = \"pearson\")\n#\n# get star symbols for p<0.001\nKTCcast.cor$P <- ifelse(KTCcast.cor$P <= 0.001,\"*\",\"\")\n#\n# make a table that combines correlation values\n# and star symbols\nKTCcast.cor_sum <- data.frame(matrix(paste(matrix(\n                   round(unlist(KTCcast.cor$r),3), \n                   nrow(KTCcast.cor$r), \n                   ncol(KTCcast.cor$r)), \n                   matrix(unlist(KTCcast.cor$P), \n                   nrow(KTCcast.cor$P), \n                   ncol(KTCcast.cor$P)), sep=\"\"), \n                   nrow(KTCcast.cor$r), \n                   ncol(KTCcast.cor$r)))        \n\n# assign column and row names to this new table\ncolnames(KTCcast.cor_sum) <- colnames(KTCcast.cor$r)\nrownames(KTCcast.cor_sum) <- colnames(KTCcast.cor$r)\n#\n# have a look at the table\nKTCcast.cor_sum\n#\n# save as CSV so you can view/copy/paste from Excel\nwrite.csv(KTCcast.cor_sum, file=\"KTC_ICP_Data_correlations.csv\")\n#\n# find out where that file was created\ngetwd()\n#\n# Finally, the last step for the ICP data, is to calculate \n# how precise our measurements were. This is a vital piece \n# of information  that must accompany all reports of \n# elemental analsyes. You  have to include it in your \n# lab report.\n# \n# We are going to calculate a measure of the spread of our\n# element concentrations for every element on groups of samples \n# that we can consider duplicates. We have two groups, the \n# blanks, which are simply our reagents and theoretically\n# identical in chemical compositions. And the replicates, \n# where we measured the same sample twice. We will calculate \n# standard error of measurement for every element for the \n# the blanks and then for the replicates. Your lab report \n# will include whichever is the highest standard error \n# (typically the replicates) as your precision value. \n# This highest standard error value is the lowest precision\n# we can be confident that we've obtained.\n#\n# First, let's calculate the standard error for the \n# measurement of each element in all the blanks\nblanksub <- as.data.frame(na.omit(cbind(as.matrix(blanks$Sample.ID), \n                                        as.matrix(blanks$Elem),\n                                        blanks$Conc..Samp.1)))\ncolnames(blanksub) <- c(\"Sample.ID\", \"Element\", \"ugL\")\nblanksub$ugL <- abs(as.numeric(as.character(blanksub$ugL)))\nblankscast <- dcast(blanksub, Sample.ID ~ Element, value.var=\"ugL\")\n# get rid of sample IDs by putting them into rownames\nrow.names(blankscast) <- blankscast$Sample.ID\n# delete sample ID column\nblankscast <- blankscast[,-1]\n# calculate standard errors for each element\nblank.se <- data.frame(lapply(blankscast, \n                              function(x) sqrt(var(x)/length(x))))\n# Here's the maximum standard error out of all the elements \n# in the blanks, we can quote this number as our precision in ppm\nmax(blank.se)\n# And just for interest, here's the element that has this\n# highest precision value\nwhich.max(blank.se[1,])\n#\n# Now let's find the precision value for each set \n# of replicates\nrepsub <- as.data.frame(na.omit(cbind(as.matrix(replicates$Sample.ID), \n                                      as.matrix(replicates$Elem), replicates$Conc..Samp.1)))\ncolnames(repsub) <- c(\"Sample.ID\", \"Element\", \"ugL\")\nrepsub$ugL <- abs(as.numeric(as.character(repsub$ugL)))\nrepcast <- dcast(repsub, Sample.ID ~ Element, value.var=\"ugL\")\nrow.names(repcast) <- repcast$Sample.ID\nrepcast <- repcast[,-1]\n# now we need to get the matching pair out of \n# the regular samples\ninstall.packages(\"stringr\")\nlibrary(stringr)\n# get the sample ID without the word 'replicate'\nblanknames <- str_extract(rownames(repcast), \".*[^_replicate]\")\n# make a table of the regular samples and their \n# untransformed element concentrations\nKTCraw <- as.data.frame(na.omit(cbind(as.matrix(KTC_ICP$Sample.ID), \n                        as.matrix(KTC_ICP$Elem),  KTC_ICP$Conc..Samp.1)))\ncolnames(KTCraw) <- c(\"Sample.ID\", \"Element\", \"ugL\")\nKTCrawcast <- dcast(KTCraw, Sample.ID ~ Element, value.var=\"ugL\")\nrow.names(KTCrawcast) <- KTCrawcast$Sample.ID\nKTCrawcast <- KTCrawcast[,-1]\n# subset just the regular samples that match \n# names with the blank samples\nKTCcastblanks <- subset(KTCrawcast,  rownames(KTCrawcast) %in% blanknames)\n# make a new table that combines replicates\n# and the matching regular sample\nallreplicates <- rbind(KTCcastblanks, repcast)\nallreplicates$sample <- rownames(allreplicates)\n# simplify the sample names for the SE\n# calculation, and calculate SE\nallreplicates$sample <- str_extract(allreplicates$sample, \".*[^_replicate]\")\nallreplicates.se <- aggregate(allreplicates[,c(1:ncol(allreplicates)-1)], \n                              list(allreplicates$sample), function(x) sqrt(var(x)/length(x)))\n# find the max standard error amongst the replicates\nmax(allreplicates.se[,c(2:ncol(allreplicates.se))])\n# find which element has that max SE\nwhich.max(allreplicates.se[1,])\n#\n# tidy up by closing plots\ndev.off()\n#\n#\n# And that's all for the ICP data. Now onto the LPSA data...\n#\n############################################################\n############################################################\n# 2. Summarise, analyse and visualise the LPSA data\n#\n# This code will get you from the raw data directly out of the instrument\n# to visualisations, summaries and statistics that you can use in your\n# lab report. You should have a close read of the journal articles that\n# we discussed in the granulometry seminar to see how to use these data.\n# This is a particularly good one that you should look at:\n#\n# Westaway, K. E., T. Sutikna, et al. (2009). \"Reconstructing \n# the geomorphic #history of Liang Bua, Flores, Indonesia: \n# a stratigraphic interpretation of the occupational environment.\" \n# Journal of Human Evolution 57(5): 465-483.\n#\n# LEt's get the data from the txt file and put it in an R object\n# ready to work on. Note that this line will pop up a window \n# that will be hidden behind the RStudio window. You may need\n# to minimise RStudio to find the window where you will select\n# the file.\n#\n# When you find the 'Select file' window, open this file:\n# KTC-LPSA-data.txt\n#\nKTC_LPSA_raw <- read.csv(file.choose(), header=FALSE, stringsAsFactors = FALSE)\n#\n# This object \"KTC_LSPA\" has the raw output from the instrument\n# which means it has a lot of stuff we don't need. Let's\n# delete the first row and first 23 columns to get only sample \n# names, size classes and sample data. We're also removing\n# column 25 and the very last column because they are empty\nKTC_LPSA <- KTC_LPSA_raw[-1,-c(1:22, 25, ncol(KTC_LPSA_raw))]\n#\n# Fill first column with numbers and give it a name, this\n# will be useful later\n#\nKTC_LPSA[,1] <- c(1:nrow(KTC_LPSA))\nnames(KTC_LPSA)[1]<-c(\"num\") \n#\n# Convert a few errant characters to numbers. This is a bug\n# of the import process where some numbers are not\n# formatted as numbers.\nKTC_LPSA[,c(3:ncol(KTC_LPSA))] <- as.numeric(as.character(unlist(KTC_LPSA[,c(3:ncol(KTC_LPSA))])))\n#\n# Reshape to long form to prepare for statistical analysis,\n# this may take a few seconds...\nKTC_l <- reshape(KTC_LPSA, idvar=1:2,\n                 varying=list(size=colnames(KTC_LPSA[seq(from=3, \n                              to=ncol(KTC_LPSA), by=3)]), \n                              meas=colnames(KTC_LPSA[seq(from=4, \n                              to=ncol(KTC_LPSA), by=3)])), \n                 direction=\"long\")\n#\n# strip off the numbers indicating replicates of the same sample\n# so we can use the sample ID as the group label for doing \n# stats by groups\ninstall.packages(\"stringr\")\nlibrary(stringr)\nKTC_l$V24 <-  str_extract(KTC_l$V24, \".+[^[:punct:]{1}[:digit:]{1}[:punct:]{1}][^rs]\")\n#\n#\n# get averages of multiple runs on the same sample\nKTC_l_a <- aggregate(KTC_l$V27, list(sample=KTC_l$V24, size=KTC_l$V26), mean)\nKTC_l_a <- subset(KTC_l_a, KTC_l_a$x != \"NA\")\n#\n# Rename measurement variable for consistency, ready for plots\n# and tables\nKTC_l_a$q <- KTC_l_a$x\n#\n# Put the sample names in order, first inspect the levels\nlevels(as.factor(KTC_l_a$sample))\n# \n# now reorder them, I've already done the hard work for you\n# here, just click through...\nKTC_l_a$sample <- factor(as.factor(KTC_l_a$sample), \n                         levels(as.factor(KTC_l_a$sample))\n                         [c(9,1,2,3,4,5,7,8,6)])\n#\n# Check that it worked ok, the sample names should be in \n# order now\nlevels(KTC_l_a$sample)\n#\n# Now we are ready to visualise the LPSA data. We'll look at\n# how to do all samples together, then just one by itself.\n#\n# Make a plot to have a quick look, all samples overlaid\nlibrary(ggplot2)\nggplot(KTC_l_a, aes(group = sample))+ geom_line(aes(x=size, y=q, colour = sample)) \n#\n# And for a different perspective, plot each sample \n# separately, one above the other \nggplot(KTC_l_a, aes(size, q)) + geom_line() + facet_grid(sample ~ .)\n#\n# Plot an individual sample, go ahead and change the\n# sample name to get different samples\nggplot(subset(KTC_l_a, KTC_l_a == \"KTC_A6\"), \n       aes(size, q)) + geom_line()\n#\n# Now we can do some basic statistical analysis of the LPSA\n# data. We will use a package dedicated to granulometry to \n# calculate typical summary statistics of particle sizes. \n# We'll get these into a table in Excel so you can put them \n# into your lab report. \n#\n# First we need to get the data in the shape that the grain \n# size stats package is expecting. Let's cast into a wide\n# table of sample by size class. \n# \ninstall.packages(\"reshape2\")\nlibrary(reshape2)\n#\nKTC_l_a <- KTC_l_a[ order(-KTC_l_a$size), ]\nKTC_cast <- t(dcast(KTC_l_a, sample ~ size, value.var=\"q\"))\ncolnames(KTC_cast) <- KTC_cast[1,]\nKTC_cast <- KTC_cast[-1,]\n#\n# We need to add an empty row at the bottom to \n# make the stats work\nz <- c(rep(0, ncol(KTC_cast)))\nKTC_cast <- data.frame(rbind(KTC_cast,z))\nrownames(KTC_cast)[nrow(KTC_cast)] <- c(\"0\")\nKTC_cast[,1:ncol(KTC_cast)] <- as.numeric(as.character(unlist(KTC_cast[,1:ncol(KTC_cast)])))\n#\n# Now we will install grain size stats package\ninstall.packages(\"G2Sd\")\nlibrary(\"G2Sd\")\n#\n# This next line will make a table of commonly used \n# grain size stats. Note that when modes is set to TRUE, \n# then you have to use mouse to click on the modal \n# point for each plot, then press escape to move on to \n# the next sample's plot. Currently I've set the function\n# to modes=FALSE so you don't have to worry about that.\n# After the last sample all the data will be generated. \n# Definitions of the  terms used can be found here\n# http://cran.r-project.org/web/packages/G2Sd/G2Sd.pdf or by \n# typing ?granstat at the R prompt\nKTC_granstat <- as.data.frame(t(granstat(KTC_cast, statistic=\"arithmetic\", aggr=TRUE, modes=TRUE)))\n#\n# Now save the output of the statistical analysis\n# as a csv file to open in Excel for easy copy-pasting. Note\n# that the output is very extensive and you must not \n# include all of it in your lab report. Be selective about\n# what you take from this table to include in your lab report.\nwrite.csv(KTC_granstat, file=\"KTC_granstat.csv\")\n#\n# find the folder where this new file was created\ngetwd()\n#\n# tidy up by closing plots\ndev.off()\n#\n############################################################\n############################################################\n#\n# 3. Summarise, analyse and visualise the basic data\n#    (ie. everything else, pH, EC, SOM, CO3, etc.)\n#\n# Now let's bring in our basic sediment data, which are pH, \n# EC, mag sus and so on. I've already extracted the data you\n# need from the google sheet, so no need to fuss around with \n# that. \n#\n# Get the data from the CSV file and put it in an R object\n# ready to work on. Note that this line will pop up a window \n# that will be hidden behind the RStudio window. You may need\n# to minimise RStudio to find the window where you will select\n# the file.\n#\n# When you find the 'Select file' window, open this file:\n# KTC-basic-data.csv\n\nKTC_basic_data <- read.csv(file.choose(), header = TRUE, stringsAsFactors = FALSE)\n#\n# Summarising these data can be done just by looking at the \n# table, you can see the max and min values easily. \n#\n# We will analyse these data by making a table of correlations\n# so we can see which of the variables are correlated, and \n# the strength, direction and significance of those correlations. \n# This an excellent way to analyse and summarise these data. I\n# recommend you include this analysis in your lab report. \n#\n# Calculate the correlation matrix (the steps here are the\n# same as what we did for the cor matrix for the ICP data)\nKTC_basic_data_m <- as.matrix(KTC_basic_data[,-1])\n# install package needed for correlation function \ninstall.packages(\"Hmisc\")\nlibrary(Hmisc)\nKTCbasic.cor <- rcorr(KTC_basic_data_m, type = \"pearson\")\n#\n# Get star symbols for p<0.001\nKTCbasic.cor$P <- ifelse(KTCbasic.cor$P <= 0.001,\"*\",\"\")\n#\n# Make a table that combines correlation values\n# and star symbols\nKTCbasic.cor_sum <- data.frame(matrix(paste(matrix(\n  round(unlist(KTCbasic.cor$r),3), \n  nrow(KTCbasic.cor$r), \n  ncol(KTCbasic.cor$r)), \n  matrix(unlist(KTCbasic.cor$P), \n  nrow(KTCbasic.cor$P), \n  ncol(KTCbasic.cor$P)), sep=\"\"), \n  nrow(KTCbasic.cor$r), \n  ncol(KTCbasic.cor$r)))        \n\n# assign column and row names to this new table\ncolnames(KTCbasic.cor_sum) <- colnames(KTCbasic.cor$r)\nrownames(KTCbasic.cor_sum) <- colnames(KTCbasic.cor$r)\n#\n# have a look at the table\nKTCbasic.cor_sum\n#\n# save as CSV so you can view/copy/paste from Excel. This is \n# a table that I hope you will include in your lab report!\nwrite.csv(KTCbasic.cor_sum, file=\"KTC_ICP_Data_correlations.csv\")\n#\n# find out where that file was created\ngetwd()\n#\n#\n# Now we've done a bit of summarising and analysis of these \n# basic data, let's look at visualising them. We will do three \n# methods. First, a quick look at how all the variables\n# relate to each other.\n#\n# This will draw a basic scatterplot matrix of all the basic\n# data, similar to what we did for the quiz.\npairs(KTC_basic_data[,-1])\n#\n# This is good for a quick and general look to see what's going\n# on. It's an exploratory method, so not something you should\n# put in your lab report. \n#\n# The second method is biplots for pairs of variables that you\n# found to be interesting in the correlation matrix above. \n# Once you've identified interesting relationships through\n# the correlation table and the scatterplot matrix above,\n# you can use this method to plot them to display in your \n# lab report\n############################################################\n#\n# optional more fancy scatterplot matrix\n# modified from http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=137\n#\npanel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor)\n{\n  usr <- par(\"usr\"); on.exit(par(usr))\n  par(usr = c(0, 1, 0, 1))\n  r <- (cor(x, y))\n  txt <- format(c(r, 0.123456789), digits=digits)[1]\n  txt <- paste(prefix, txt, sep=\"\")\n  if(missing(cex.cor)) cex <- 0.8/strwidth(txt)\n  \n  test <- cor.test(x,y)\n  # borrowed from printCoefmat\n  Signif <- symnum(test$p.value, corr = FALSE, na = FALSE,\n                   cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n                   symbols = c(\"***\", \"**\", \"*\", \".\", \" \"))\n  \n  text(0.5, 0.5, txt, cex = cex * abs(r))\n  text(.8, .8, Signif, cex=cex, col=2)\n}\npairs(KTC_basic_data[,-1], \n      lower.panel=panel.smooth, upper.panel=panel.cor)\n#\n# Here's three ways to do the biplots, pick the one you like\n# best for your lab report.\n#\n# First is the simplest plot function. You can go ahead and \n# change the variable names in the next few lines to make plots\n# with other variables. In this example I put pH and SOM, but \n# you should replace them with the ones you're interested in\n# you can see the variable names in this table:\nKTC_basic_data\n#\n# So let's make the basic plot\n#\n# we'll assign some variables with nice short names for this\n# plot to save lots of retyping. To change the variables for this\n# simple plot example, just change them in the four lines below\nx <- as.numeric(as.character(KTC_basic_data[,\n     which(colnames(KTC_basic_data)==\"SOM\")]))\ny <- as.numeric(as.character(KTC_basic_data[,\n     which(colnames(KTC_basic_data)==\"X.fd\")]))\n#\n# To expclude one sample from the group, for exmample if you\n# suspect it might be an outlier and you want to see what \n# the effect is if it is removed, try this for both x and y \n# above (run KTC_basic_data at the R promtp to see the \n# exact spelling of the Sample.ID)\n# \n# x <-  as.numeric(as.character(KTC_basic_data[\n#      -which(KTC_basic_data$Sample.ID==\"B-1\"),\n#      which(colnames(KTC_basic_data)==\"SOM\")]))\n# y <-  as.numeric(as.character(KTC_basic_data[\n#      -which(KTC_basic_data$Sample.ID==\"B-1\"),\n#      which(colnames(KTC_basic_data)==\"X.fd\")]))\n#\n#\nplot(x, y,\n    xlab=\"Soil organic matter\",   # label for x axis\n    ylab=\"pH\",  # label for y axis\n    las=1) # turns the numbers on the y axis the right-way\n#\n# This first method is fine for most people. But we can make a few simple \n# improvements to this to get to publication-quality\n#\n# We'll strip off the \"KTC\" from the sample IDs because that's\n# redundant\nlibrary(stringr)\nKTC_basic_data$Sample.ID <- str_extract(KTC_basic_data$Sample.ID, \"[^KTC-].*\")\n#\n# Here's the second biplot method. Let's go full Tufte-mode \n# and change the axes to # rug plots to make \n# what Tufte calls a \"dot-dash-plot\". \n# We'll redo the previous plot and then replace the axis \n# lines with rug plots.\n#\n# get rid of the last plot...\ndev.off()\n#\nplot(x, y,\n     xlab=\"Soil organic matter\",   # label for x axis\n     ylab=\"X.fd\",  # label for y axis\n     las=1,   # turns the numbers on the y axis the right-way\n     pch=\"\",  # stops it from drawing any data points\n     xlim=c(min(x), max(x)), # tidy the x-axis\n     ylim=c(min(y), max(y)), # tidy the x-axis\n     bty=\"n\", # remove the box surrounding the plot\n     axes=FALSE)  # remove the axis lines\n#\n# You should see hardly anything, just axis labels...\n# Now we'll put on some text as the data points\ntext(x, y, KTC_basic_data$Sample.ID, cex = 1.5)\n#\n# the cex value in the line above specifies the \n# size of the text, feel free to adjust it\n#\n# Now we'll put the rug plots to show the marginal \n# distributions of our variables\nrug(x, side=1, line=0.9)\nrug(y, side=2, line=0.9)\n#\n# put in the axis numbers along each \n# axis but with no axis lines\naxis(1, lwd=0)\naxis(2, lwd=0)\n#\n# Now we're looking more fancy. We can add to this plot\n# a line and some summary statistics summarising the \n# the relationship between these two variables.\n#\n# Correlation is the most commonly \n# used statistical technique to investigate \n# any kind of data. A correlation is a\n# single number that describes the degree \n# of relationship between two variables, \n# which we can label X and Y, just for \n# example. Correlations can suggest possible \n# causal relationships but the statistical \n# test alone is not sufficient to demonstrate \n# the relationship, you also need explain it \n# with some logic and details of the likely \n# causal connection. The answer to the \n# question of how much are the two variables\n# correlated comes from two numbers: \n#\n# r, or the coefficient of correlation. r \n# takes on a value in the range from -1 to 1. \n# This measure will show whether the correlation\n# between variables is strong (close to 1 or -1),\n# weak or non-existent (zero or nearly zero) \n# and indicate the direction of this correlation. \n# A positive value for r means that larger \n# X values go along with larger Y values while \n# a negative r value means when X values get \n# larger, the Y values get smaller. \n#                                                                                                                                                                                                                                                                       # p or the probability value. This value \n# tells you the probability of the r-value \n# in a collection of random data in which you'd \n# have no reason to suspect any relationship. A \n# p-value of 0.05 indicates a 5% chance that \n# results you are seeing would have come up in \n# a random dataset, so you can say with a 95% \n# probability of being correct that you are \n# observing a real relationship in your data. \n# Scholarly convention tends to hold p values\n# of less than 0.05 as indicators of a \n# statistically signficant relationship. Above \n# 0.05 is not significant. \n#\n# Note that the size of the p-value in your \n# correlation analysis says nothing about the \n# strength of the relationship between your \n# two variables - it is possible to have a \n# highly significant result (very small p-value, \n# eg. p = 0.001) for a miniscule effect (ie. \n# an r value close to zero) that might not \n# be of any real-world importance or archaeological \n# meaning. \n#\n# You need to use your common sense \n# when interpreting your correlation data; don't \n# just mindlessly copy and paste them and get \n# excited when r is big or p-values \n# are small. Ensure that you're only \n# getting excited about data that make \n# sense in the real world.                                                                                                                                                                                                                                                        \n#\n# We will calculate the correlation coeffient\n# to show on the plot and get the probability \n# value of the null hypothesis (that the \n# correlation is due to random \n# processes that we don't care about). This value will \n# help us test the hypothesis that \n# the overall slope of the linear \n# regression in 0 (ie. there is no \n# relationship between the two variables). \n# A line with slope 0 is horizontal, \n# which means that Y does not depend on X at all.   \n#\n# These next lines will calculate and print the correlation \n# coefficent and pvalue at the bottom of the plot\ntitle(sub=paste(\"r=\", round(cor(x,y), 4), \n      \",   p-value=\", round(anova(lm(y~x))$'Pr(>F)'[1], \n                                      4))) \n#\n# Remember that a correlation is considered to be interesting\n# only when the p value is less than 0.05. If it's greater\n# than 0.05, then the correlation is most likely due to\n# chance and not suggestive of anything interesting. \n#\n# We can also add the line representing the \n# regression model. This line is a \n# series of the means of the expected \n# value of the Y variable for each \n# value of the X variable. If the line \n# is close to horizontal, you've got \n# nothing. If the line's slope is close \n# to 45 degrees, you've got a \n# relationship between the two variables. \nabline(lm(y~x), col=\"grey\")\n#\n# Here's the third biplot method, this time using ggplot\n# The numbers on the axes are the minimum, lower-hinge, median, \n# upper-hinge and maximum of the data. This is Tukey's five\n# number summary. This plot has a very high data:ink\n# ratio, without looking too ultra- minimalist.\n#\n# It's quite a bit more involved since there's a lot of\n# customisation relative to the base plot fuction. You're \n# welcome to adjust the settings to get what you want.\n# Unfortunately you have to laboriously change\n# the variable name in several places \n# each time you want to change it. \n#\nlibrary(ggplot2)\nggplot(KTC_basic_data, \n  aes(pH, EC, label=KTC_basic_data$Sample.ID)) +        \n  # make the basic plot object\n  scale_x_continuous(limit=c(min(KTC_basic_data$pH),\n                             max(KTC_basic_data$pH)), \n  breaks=round(fivenum(KTC_basic_data$pH),1)) +     \n  # set the locations of the x-axis labels\n  xlab(paste(\"pH (r = \", round(cor(x,y),4),\", p-value = \", \n  round(anova(lm(KTC_basic_data$pH~KTC_basic_data$EC\n                 ))$'Pr(>F)'[1],4),\")\",sep=\"\")) +      \n  # paste in the r and p values with the axis label\n  scale_y_continuous(limit=c(min(KTC_basic_data$EC),\n                             max(KTC_basic_data$EC)),\n  breaks=round(fivenum(KTC_basic_data$EC),1)) +     \n  # set the locations of the y-axis labels\n  ylab(\"Electrical Conductivity (uS)\") +                   \n  geom_text() + # specify that the data points are the sample names\n  geom_rug(size=0.1) +   # specify that we want the rug plot\n  geom_smooth(aes(group = 1), method=\"lm\", fill=\"grey80\", alpha=0.05,\n  colour=\"grey80\") + \n  # specify a line of best fit calculated by a linear model\n  # fill is the colour of the standard error area\n  # with fill set it to \"white\" it wont show at all\n  # colour is the colour of the line\n  opts(panel.background = theme_blank(),              \n  # suppress default background\n  panel.grid.major = theme_blank(),              \n  # suppress default major gridlines\n  panel.grid.minor = theme_blank(),              \n  # suppress default minor gridlines\n  axis.ticks = theme_blank(),                    \n  # suppress tick marks\n  axis.title.x=theme_text(size=15),              \n  # increase axis title size slightly  \n  axis.title.y=theme_text(size=15, angle=90),    \n  # increase axis title size slightly and rotate\n  axis.text.x=theme_text(size=12),              \n  # increase size of numbers on x-axis\n  axis.text.y=theme_text(size=12))              \n  # increase size of numbers on y-axis\n#\n# So there you go, three ways to make biplots. Pick your\n# favourite for your lab repot. \n#\n# You may end up making a lot of charts like this to explore \n# your data. But do not put them all in your lab report. \n# You may include a table of all the r2 and p-values if \n# you wish, but you should only include one or more of \n# these biplots if they are especially interesting and\n# you discuss it in your text. Don't just dump charts in \n# your lab report because you made them! Inspect them all \n# carefully and only use figures that you need to refer to \n# in the text of your report.\n#\n# The third and final visualisation for these basic data is\n# the stratigraphic plot, showing how the data change down \n# the deposit (ie. through time). We will also tack on\n# two variables from the grain size data to this plot.\n#\n# Let's get just the mean and standard deviation from the\n# grain size data ready to attach it to the table of \n# basic sediment data to make a stratigraphic plot\nKTC_granstat_subset <- as.data.frame(cbind(as.numeric(as.character(KTC_granstat[,\"mean.arith\"])), as.numeric(as.character(KTC_granstat[,\"sd.arith\"]))))\nnames(KTC_granstat_subset) <- c(\"mean.arith\", \"sd.arith\")\nrow.names(KTC_granstat_subset)  <- row.names(KTC_granstat)\n#\n# Let's have a look at the order of the samples in our basic\n# data\nlevels(as.factor(KTC_basic_data$Sample.ID))\n#\n# And re-order them to make the plot look right\nKTC_basic_data$Sample.ID <- factor(as.factor(KTC_basic_data$Sample.ID), \n                              levels(as.factor(KTC_basic_data$Sample.ID))\n                              [c(9,1,2,3,4,5,7,8,6)])\n#\n# check that it worked ok\nlevels(KTC_basic_data$Sample.ID)\n#\n# now join the granstat data to the basic data\nKTC_basic_granstat <- cbind(KTC_basic_data, KTC_granstat_subset)\n#\n# add depth values in m below the surface\nKTC_basic_granstat$depths <- c(0.02, 0.09, 0.22, 0.30, 0.40, 0.53, 0.72, 0.92, 1.10)\n#\n# Now can make the stratigraphic plot. Choose your favourite,\n# I've shown two methods below.\n#\ninstall.packages(c(\"vegan\", \"princurve\"))\ninstall.packages(\"analogue\", repos=\"http://R-Forge.R-project.org\")\nlibrary(analogue)\n# Make some data objects that the plot function needs\ndepths <- KTC_basic_granstat$depths\n# Check that all the numbers really are numbers\nKTC_basic_granstat[,c(2:ncol(KTC_basic_granstat))] <- as.numeric(as.character(unlist(KTC_basic_granstat[,                                                               c(2:ncol(KTC_basic_granstat))])))\n# subset just the data, minus sample names\nKTC_strat_plot <- KTC_basic_granstat[,-c(1,10)]\n# and finally make the plot\nStratiplot(KTC_strat_plot, y = depths, \n           type = c(\"h\", \"l\"), \n           ylab = \"depth below surface (m)\",\n           varTypes = \"absolute\")\n#\n# and now you should have a nice stratigraphic plot\n# containg basic geoarchaeological data arranged by depth\n#\n# Here's a second method to do the same thing, \n# just a different style. Pick your favorite\ninstall.packages(\"rioja\")\nlibrary(rioja)\nstrat.plot(KTC_strat_plot, \n           yvar = depths, \n           y.rev = TRUE, \n           ylabel = \"Depth below surface (m)\")\n#\n#\n#\n# And that's the end of the tour. We haven't included in here the \n# XRD data, which I think you can just put in a simple table \n# in your lab report. Let me know if I've missed anything \n# or you can't get things to work!",
    "created" : 1452204951065.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3233640201",
    "id" : "72FE8B79",
    "lastKnownWriteTime" : 1452117323,
    "last_content_update" : 1452117323,
    "path" : "C:/Users/marwick/Downloads/Geoarch_grand_tour.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}